{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science: Web Scrapping \n",
    "#### By: Javier Orduz\n",
    "<!--\n",
    "<img\n",
    "src=\"https://jaorduz.github.io/images/Javier%20Orduz_01.jpg\" width=\"50\" align=\"center\">\n",
    "-->\n",
    "\n",
    "[license-badge]: https://img.shields.io/badge/License-CC-orange\n",
    "[license]: https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en\n",
    "\n",
    "[![CC License][license-badge]][license]  [![DS](https://img.shields.io/badge/downloads-DS-green)](https://github.com/Earlham-College/DS_Fall_2022)  [![Github](https://img.shields.io/badge/jaorduz-repos-blue)](https://github.com/jaorduz/)  ![Follow @jaorduc](https://img.shields.io/twitter/follow/jaorduc?label=follow&logo=twitter&logoColor=lkj&style=plastic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Table of contents</h1>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#about_WS\">About Web Scrapping</a></li>\n",
    "<!--         <ol>\n",
    "            <li><a href=\"about_dataset\"></a> Libraries, modules, and data set</li>\n",
    "        </ol> -->\n",
    "        <li><a href=\"#theWebsite\">The Website</a></li>\n",
    "        <li><a href=\"#exercies\">Exercises</a></li>\n",
    "<!--         <li><a href=\"#practice\">Practice</a></li> -->\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping\n",
    "\n",
    "Web scraping is the process of systematically extracting data from websites for analysis using custom software tools. Python has become one of the predominant programming languages used for web scraping due to its accessibility, flexibility, and abundance of specialized libraries dedicated to the task.\n",
    "\n",
    "### Goals with this NB\n",
    "\n",
    "- Provide an overview about WS.\n",
    "- We will use a website to explore its information.\n",
    "\n",
    "### Characteristics with Python\n",
    "\n",
    "- **Simplicity:** Python is relatively easy to learn, even for beginners. This makes it a good choice for those who are new to web scraping.\n",
    "- **Flexibility:** Python has a large number of libraries and tools available for web scraping. This makes it possible to scrape data from a wide variety of websites.\n",
    "- **Efficiency:** Python can be very efficient at scraping data. This is especially important when scraping large amounts of data.\n",
    "\n",
    "### Remarks\n",
    "\n",
    "- It can be time-consuming to write web scraping scripts.\n",
    "- Web scraping can be fragile. Websites can change their HTML code at any time, which can break your scraping scripts.\n",
    "- Web scraping can violate the terms of service of a website.\n",
    "\n",
    "- Web scrapers built solely for performance may opt for languages like **Java, C#,** or **Go** over Python. Though Python scrapers can be highly performant, compiled languages exceed Python's efficiency for particularly resource-intensive jobs. \n",
    "- **Legality and ethics** are also persistent issues for web scraping in general, regardless of language choice. Scrapers introduce scalability concerns, can fail without warning, and may violate a website's terms of service if used carelessly or excessively.\n",
    "\n",
    "Overall, web scraping with Python is a powerful tool that can be used to collect data from a wide variety of websites. However, it is important to be aware of the potential risks involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# The website\n",
    "\n",
    "Turns out that https://www.nobelprize.org/prizes/lists/all-nobel-prizes/ has the data we want. \n",
    "\n",
    "Let's take a look at the [website](https://www.nobelprize.org/prizes/lists/all-nobel-prizes/) and to look at the underhood HTML\n",
    "<!---\n",
    ": right-click and click on `inspect` . Try to find structure in the tree-structured HTML.\n",
    "--->\n",
    "\n",
    "Consider the `nobelprize.org` server is a little slow sometimes. \n",
    "\n",
    "<!---\n",
    "Fortunately, the Internet Archive periodically crawls most of the Internet and saves what it finds. (That's a lot of data!) So let's grab the data from the Archive's \"Wayback Machine\" (great name!).\n",
    "We'll just give you the direct URL, but at the very end you'll see how we can get it out of a JSON response from the Wayback Machine API.\n",
    "--->\n",
    "\n",
    "#### Revise the meaning\n",
    "What is a this Response [200]? Let's google: [`response 200 meaning`](https://www.google.com/search?q=response+200+meaning&oq=response+%5B200%5D+m&aqs=chrome.1.69i57j0l5.6184j0j7&sourceid=chrome&ie=UTF-8). All possible codes [here](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_url = 'https://www.nobelprize.org/prizes/lists/all-nobel-prizes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = requests.get(snapshot_url)\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(snapshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to request \"www.xoogle.be\"? What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot_url2 = 'http://web.archive.org/web/20180820111639/https://www.xoogle.be'\n",
    "snapshot = requests.get(snapshot_url2)\n",
    "snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important information\n",
    "Always remember to “not to be evil” when scraping with requests! If downloading multiple pages (like you will be on HW1), always put a delay between requests (e.g, `time.sleep(1)`, with the `time` library) so you don’t unwittingly hammer someone’s webserver and/or get blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = requests.get(snapshot_url)\n",
    "raw_html = snapshot.text\n",
    "print(raw_html[:560])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions\n",
    "You can find specific patterns or strings in text by using Regular Expressions: This is a pattern matching mechanism used throughout Computer Science and programming (it's not just specific to Python). Some great resources that we recommend, if you are interested in them (could be very useful for a homework problem):\n",
    "- https://docs.python.org/3.3/library/re.html\n",
    "- https://regexone.com\n",
    "- https://docs.python.org/3/howto/regex.html.\n",
    "\n",
    "Specify a specific sequence with the help of regex special characters. Some examples: \n",
    "- ```\\S``` : Matches any character which is not a Unicode whitespace character\n",
    "- ```\\d``` : Matches any Unicode decimal digit \n",
    "- ```*``` : Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible.\n",
    "\n",
    "**Let's find all the occurances of 'Marie' in our raw_html:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'Marie',raw_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using ```\\S``` to match 'Marie' + ' ' + 'any character which is not a Unicode whitespace character':**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(r'Marie \\S',raw_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all our data in the notebook. Unfortunately, it is the form of one really long string, which is hard to work with directly. This is where BeautifulSoup comes in.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the HTML with BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(raw_html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key BeautifulSoup functions we’ll be using in this section:\n",
    "- **`tag.prettify()`**: Returns cleaned-up version of raw HTML, useful for printing\n",
    "- **`tag.select(selector)`**: Return a list of nodes matching a [CSS selector](https://developer.mozilla.org/en-US/docs/Learn/CSS/Introduction_to_CSS/Simple_selectors)\n",
    "- **`tag.select_one(selector)`**: Return the first node matching a CSS selector\n",
    "- **`tag.text/soup.get_text()`**: Returns visible text of a node (e.g.,\"`<p>Some text</p>`\" -> \"Some text\")\n",
    "- **`tag.contents`**: A list of the immediate children of this node\n",
    "\n",
    "You can also use these functions to find nodes.\n",
    "- **`tag.find_all(tag_name, attrs=attributes_dict)`**: Returns a list of matching nodes\n",
    "- **`tag.find(tag_name, attrs=attributes_dict)`**: Returns first matching node\n",
    "\n",
    "BeautifulSoup is a very powerful library -- much more info here: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's practice some BeautifulSoup commands..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print a cleaned-up version of the raw HTML**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the first “title” object** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the text of first “title” object** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.title.parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"search-mobile-trigger-js\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "\n",
    "1. Run this entire Notebook with the previous link revise the results, do the same thing with this link \n",
    "`http://web.archive.org/web/20180820111639/https://www.nobelprize.org/prizes/lists/all-nobel-prizes/` and figure out why there are differences if any.\n",
    "\n",
    "1. Find a different website and run the same previous commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
