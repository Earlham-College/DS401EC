{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bd7dd63-41ae-4d54-af19-d2f8ce7e0673",
   "metadata": {},
   "source": [
    "# Statistics for Data Science [DS401]\n",
    "## Web Scrapping\n",
    "#### By: Javier Orduz\n",
    "\n",
    "[licenseBDG]: https://img.shields.io/badge/License-CC-orange?style=plastic\n",
    "[license]: https://creativecommons.org/licenses/by-nc-sa/3.0/deed.en\n",
    "\n",
    "[mywebsiteBDG]:https://img.shields.io/badge/website-jaorduz.github.io-0abeeb?style=plastic\n",
    "[mywebsite]: https://jaorduz.github.io/\n",
    "\n",
    "[mygithubBDG-jaorduz]: https://img.shields.io/badge/jaorduz-repos-blue?logo=github&label=jaorduz&style=plastic\n",
    "[mygithub-jaorduz]: https://github.com/jaorduz/\n",
    "\n",
    "[mygithubBDG-jaorduc]: https://img.shields.io/badge/jaorduc-repos-blue?logo=github&label=jaorduc&style=plastic \n",
    "[mygithub-jaorduc]: https://github.com/jaorduc/\n",
    "\n",
    "[myXprofileBDG]: https://img.shields.io/static/v1?label=Follow&message=jaorduc&color=2ea44f&style=plastic&logo=X&logoColor=black\n",
    "[myXprofile]:https://twitter.com/jaorduc\n",
    "\n",
    "\n",
    "[![website - jaorduz.github.io][mywebsiteBDG]][mywebsite]\n",
    "[![Github][mygithubBDG-jaorduz]][mygithub-jaorduz]\n",
    "[![Github][mygithubBDG-jaorduc]][mygithub-jaorduc]\n",
    "[![Follow @jaorduc][myXprofileBDG]][myXprofile]\n",
    "[![CC License][licenseBDG]][license]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf2f90f-a597-466b-9512-8335991e68e8",
   "metadata": {},
   "source": [
    "<h1>Contents</h1>\n",
    "<div class=\"alert  alert-block alert-info\" style=\"margin-top: 1px\">\n",
    "    <ol>\n",
    "        <li><a href=\"#daExploration\">Definition: Web Scrapping</a></li>\n",
    "        <li><a href=\"#showData\">Libraries for web scrapping and pandas</a></li>\n",
    "            <!-- <li><a href=\"#unData\">Import modules</a></li> -->\n",
    "         <!-- <ol>\n",
    "             <li><a href=\"#reData\">Libraries for web scrapping</a></li>\n",
    "         </ol> -->\n",
    "        <li><a href=\"#PrintoutText\">Print out the text</a></li>\n",
    "        <li><a href=\"#versions\">Versions</a></li>\n",
    "        <li><a href=\"#exercises1\">Exercises 1</a></li>\n",
    "        <li><a href=\"#regex\">Regex</a></li>\n",
    "        <li><a href=\"#manipuClean\">Using pandas, data manipulating and cleaning</a></li>\n",
    "        <li><a href=\"#exercises2\">Exercise 2</a></li>        \n",
    "        <li><a href=\"#references\">References</a></li>\n",
    "    </ol>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7046a5af-f120-4362-b069-516ebbcb4a13",
   "metadata": {},
   "source": [
    "**Web scraping** involves using a program or algorithm to extract and process data from websites in large quantities. This skill is valuable for data scientists, engineers, or anyone needing to analyze extensive datasets. When you find data online that isn’t available for direct download, web scraping with Python enables you to collect and format this data for easier analysis and use in your projects. This technique can transform unstructured web data into a format that’s ready for importing and manipulation in various analytical tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8206a-8cb2-4f5f-96e6-10eed867d4a7",
   "metadata": {},
   "source": [
    "## Libraries for web scrapping and pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330d85f9-bd7b-450d-83be-7f6f78eaad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339631dd-070a-4295-afa0-5145f91a1b02",
   "metadata": {},
   "source": [
    "## Exploring a website\n",
    "You should specify the URL containing the dataset and use ```urlopen()``` to het the html of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d992dd2a-5e9e-4b0f-af8e-3671d4b25fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "html = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcad49b-8619-46c1-bac9-7eaf9beaacce",
   "metadata": {},
   "source": [
    "After getting the **html** of the page, we should create a **Beautiful Soup object** from the html, so we should use ```BeautifulSoup()``` function. This function has two parameters one for  html and  second argument __lxml__ is the html parser, we don't need more details about these so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e65906-1b0c-4bed-b3e7-e9e25496d0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'lxml')\n",
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec3ea6-05d8-47c9-94d9-38b92f28002a",
   "metadata": {},
   "source": [
    "We can extract information form the website, e.g. getting the title as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce07d66-63dd-4cec-bb14-8d513ffd79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = soup.title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e19943-b6c7-400c-aaa8-317bd4bd49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('img').get('alt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2a5cb3-c68d-4ccf-a0ec-04efad2b2545",
   "metadata": {},
   "source": [
    "## Print out the text\n",
    "Create a variable and print the text of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa6799-ef46-4f64-bde4-6fef463f0df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd2003c-d1ae-41a9-8bb9-c2150a3eae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f196be8-a6bb-4471-80e1-5c8dc6aa9f5c",
   "metadata": {},
   "source": [
    "Extracting **html** tags within a webpage. Examples:\n",
    "```html\n",
    "a\n",
    "table\n",
    "tr\n",
    "```\n",
    "Where we will use ```a``` for hyperlinks, ```table```  for tables, ```tr``` for table rows, and so on. Check ref. [[2](https://www.w3schools.com/tags/tag_html.asp)] for more tags.\n",
    "\n",
    "We can use the ```find_all()``` method to obtain helpful  HTML tags within a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65e4ac6-a53d-4006-beee-f74fe7a23c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "all_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da6596a-5910-4804-8d8c-43cc07ad5678",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tables = soup.find_all('table')\n",
    "all_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523e9f41-1ec4-4887-87eb-b5f7fa2571ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = soup.find_all('tr')\n",
    "print(rows[:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76aac41-df7c-4906-ab42-9fcd221258e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    row_td = row.find_all('td')\n",
    "print(row_td)\n",
    "type(row_td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831d301c-9f7f-4c53-b8f1-87d35888e3b8",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "From the last output, we can see that ```html``` tags sometimes come with attributes such as ```class```, ```src```, etc. These attributes provide additional information about ```html``` elements. You can use a for loop and the ```get('\"href\")``` method to extract and print out only hyperlinks.\n",
    "\n",
    "<!-- all_links = soup.find_all(\"a\")\n",
    "for link in all_links:\n",
    "    print(link.get(\"href\")) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640eec08-f382-4224-a868-cc54fafd6a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e845fd9-781f-4ea7-8cc3-2dd21c1198ba",
   "metadata": {},
   "source": [
    "# Regular expressions (Regex)\n",
    "A regular expression is defined using **tokens that match a particular pattern** [[3](https://brightdata.com/blog/web-data/web-scraping-with-regex)].\n",
    "\n",
    "With Regex, it is common to make a lot of mistakes; take care. It requires to import\n",
    "```python\n",
    "import re\n",
    "```\n",
    "module. The following code shows how to build a regular expression that finds all the characters inside the ```< td >``` html tags and replace them with an empty string for each table row. \n",
    "1. Compile a regular expression by passing a string to match to ```re.compile()```.\n",
    "2. The dot, star, and question mark (```.*?```) will match an opening angle bracket followed by anything and followed by a closing angle bracket.\n",
    "3. It matches text in a non-greedy fashion, that is, it matches the shortest possible string. If you omit the question mark, it will match all the text between the first opening angle bracket and the last closing angle bracket.\n",
    "4. After compiling a regular expression, you can use the ```re.sub()``` method to find all the substrings where the regular expression matches and replace them with an empty string.\n",
    "\n",
    "The full code below generates an empty **list**, **extracts** text in between HTML tags for each row, and **appends** it to the assigned list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e88ca-62ff-4101-8095-10c8f9d4a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "list_rows = []\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    str_cells = str(cells)\n",
    "    clean = re.compile('<.*?>')\n",
    "    clean2 = (re.sub(clean, '',str_cells))\n",
    "    list_rows.append(clean2)\n",
    "print(clean2)\n",
    "type(clean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76669462-4c7a-4808-bc04-c9bf496ada1c",
   "metadata": {},
   "source": [
    "## Using pandas, data manipulating and cleaning\n",
    "\n",
    "The next step is to convert the list into a dataframe and get a quick view of the first 10 rows using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd1b5d-cb42-4f77-92c0-27bc57016031",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list_rows)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08a995-e964-4717-9826-c8e559e5e7ba",
   "metadata": {},
   "source": [
    "Cleaning: split the \"0\" column into multiple columns at comma position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f761653-60a6-425f-98bc-1e12b18d9d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[0].str.split(',', expand=True)\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23db66c-2a62-4bc1-9d19-9c75cf23e95e",
   "metadata": {},
   "source": [
    "The dataframe has unwanted square brackets surrounding each row. You can use the ```strip()``` method to remove the opening square bracket on column \"0.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6afdb0-aa7f-41e2-950d-a537f714974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[0] = df1[0].str.strip('[')\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b06d0-92fa-4710-b00a-9cdb9741d879",
   "metadata": {},
   "source": [
    "The table is almost adequately formatted. You can start by getting an overview of the data as shown below, and make an analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cb4bb9-4ab0-4bd5-aa1c-dd75bd772c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8181da-d765-499f-97dd-09fccadf281b",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Find a different website to do the same that we did here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb4e0d7-bc4c-42a4-91d5-a30e193a3ffd",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] https://www.datacamp.com/tutorial/web-scraping-using-python\n",
    "\n",
    "[2] https://www.w3schools.com/tags/tag_html.asp\n",
    "\n",
    "[3] https://brightdata.com/blog/web-data/web-scraping-with-regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa311a-b518-4c5b-b7a9-e7ccbf14097c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
